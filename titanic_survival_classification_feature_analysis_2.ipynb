{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9/18-9/19/23\n",
    "\n",
    "This is more code for the Kaggle Titanic Competition. Last time, I found that Sex is basically the only trait that matters. This time around, I'll see what sklearn's RandomForestClassifier shows about feature importances, and maybe try making a few feature of my own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, make_column_transformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import random\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_set = pd.read_csv(\"train.csv\")\n",
    "test_set = pd.read_csv(\"test.csv\")\n",
    "train_set.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 889 entries, 0 to 890\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Survived  889 non-null    int64  \n",
      " 1   Pclass    889 non-null    int64  \n",
      " 2   Sex       889 non-null    object \n",
      " 3   Age       712 non-null    float64\n",
      " 4   SibSp     889 non-null    int64  \n",
      " 5   Parch     889 non-null    int64  \n",
      " 6   Fare      889 non-null    float64\n",
      " 7   Embarked  889 non-null    object \n",
      "dtypes: float64(2), int64(4), object(2)\n",
      "memory usage: 62.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train_set.dropna(subset=[\"Embarked\"], inplace=True)\n",
    "survived = np.array(pd.DataFrame(train_set[\"Survived\"].copy())).ravel()\n",
    "\n",
    "ids2 = np.array(test_set[\"PassengerId\"].copy()).ravel() # contest submission requires passenger ids\n",
    "\n",
    "nec_data = train_set.copy()\n",
    "nec_data = train_set.drop(\"Name\", axis=1) # string and too individual\n",
    "nec_data = nec_data.drop(\"PassengerId\", axis=1) # too individual\n",
    "nec_data = nec_data.drop(\"Cabin\", axis=1) # too many null values\n",
    "nec_data = nec_data.drop(\"Ticket\", axis=1) # too individual and discordant\n",
    "nec_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The well-known phrase associated with the Titanic evacuation is \"Women and children first\". My previous feature importance testing found that Age, the original feature, wasn't very correlated with survival. Maybe simplifying it to child or adult will be more pertinent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22.  ]\n",
      " [38.  ]\n",
      " [26.  ]\n",
      " [35.  ]\n",
      " [35.  ]\n",
      " [28.  ]\n",
      " [54.  ]\n",
      " [ 2.  ]\n",
      " [27.  ]\n",
      " [14.  ]\n",
      " [ 4.  ]\n",
      " [58.  ]\n",
      " [20.  ]\n",
      " [39.  ]\n",
      " [14.  ]\n",
      " [55.  ]\n",
      " [ 2.  ]\n",
      " [28.  ]\n",
      " [31.  ]\n",
      " [28.  ]\n",
      " [35.  ]\n",
      " [34.  ]\n",
      " [15.  ]\n",
      " [28.  ]\n",
      " [ 8.  ]\n",
      " [38.  ]\n",
      " [28.  ]\n",
      " [19.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [40.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [66.  ]\n",
      " [28.  ]\n",
      " [42.  ]\n",
      " [28.  ]\n",
      " [21.  ]\n",
      " [18.  ]\n",
      " [14.  ]\n",
      " [40.  ]\n",
      " [27.  ]\n",
      " [28.  ]\n",
      " [ 3.  ]\n",
      " [19.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [18.  ]\n",
      " [ 7.  ]\n",
      " [21.  ]\n",
      " [49.  ]\n",
      " [29.  ]\n",
      " [65.  ]\n",
      " [28.  ]\n",
      " [21.  ]\n",
      " [28.5 ]\n",
      " [ 5.  ]\n",
      " [11.  ]\n",
      " [22.  ]\n",
      " [45.  ]\n",
      " [ 4.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [29.  ]\n",
      " [19.  ]\n",
      " [17.  ]\n",
      " [26.  ]\n",
      " [32.  ]\n",
      " [16.  ]\n",
      " [21.  ]\n",
      " [26.  ]\n",
      " [32.  ]\n",
      " [25.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [ 0.83]\n",
      " [30.  ]\n",
      " [22.  ]\n",
      " [29.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [17.  ]\n",
      " [33.  ]\n",
      " [16.  ]\n",
      " [28.  ]\n",
      " [23.  ]\n",
      " [24.  ]\n",
      " [29.  ]\n",
      " [20.  ]\n",
      " [46.  ]\n",
      " [26.  ]\n",
      " [59.  ]\n",
      " [28.  ]\n",
      " [71.  ]\n",
      " [23.  ]\n",
      " [34.  ]\n",
      " [34.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [21.  ]\n",
      " [33.  ]\n",
      " [37.  ]\n",
      " [28.  ]\n",
      " [21.  ]\n",
      " [28.  ]\n",
      " [38.  ]\n",
      " [28.  ]\n",
      " [47.  ]\n",
      " [14.5 ]\n",
      " [22.  ]\n",
      " [20.  ]\n",
      " [17.  ]\n",
      " [21.  ]\n",
      " [70.5 ]\n",
      " [29.  ]\n",
      " [24.  ]\n",
      " [ 2.  ]\n",
      " [21.  ]\n",
      " [28.  ]\n",
      " [32.5 ]\n",
      " [32.5 ]\n",
      " [54.  ]\n",
      " [12.  ]\n",
      " [28.  ]\n",
      " [24.  ]\n",
      " [28.  ]\n",
      " [45.  ]\n",
      " [33.  ]\n",
      " [20.  ]\n",
      " [47.  ]\n",
      " [29.  ]\n",
      " [25.  ]\n",
      " [23.  ]\n",
      " [19.  ]\n",
      " [37.  ]\n",
      " [16.  ]\n",
      " [24.  ]\n",
      " [28.  ]\n",
      " [22.  ]\n",
      " [24.  ]\n",
      " [19.  ]\n",
      " [18.  ]\n",
      " [19.  ]\n",
      " [27.  ]\n",
      " [ 9.  ]\n",
      " [36.5 ]\n",
      " [42.  ]\n",
      " [51.  ]\n",
      " [22.  ]\n",
      " [55.5 ]\n",
      " [40.5 ]\n",
      " [28.  ]\n",
      " [51.  ]\n",
      " [16.  ]\n",
      " [30.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [44.  ]\n",
      " [40.  ]\n",
      " [26.  ]\n",
      " [17.  ]\n",
      " [ 1.  ]\n",
      " [ 9.  ]\n",
      " [28.  ]\n",
      " [45.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [61.  ]\n",
      " [ 4.  ]\n",
      " [ 1.  ]\n",
      " [21.  ]\n",
      " [56.  ]\n",
      " [18.  ]\n",
      " [28.  ]\n",
      " [50.  ]\n",
      " [30.  ]\n",
      " [36.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [ 9.  ]\n",
      " [ 1.  ]\n",
      " [ 4.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [45.  ]\n",
      " [40.  ]\n",
      " [36.  ]\n",
      " [32.  ]\n",
      " [19.  ]\n",
      " [19.  ]\n",
      " [ 3.  ]\n",
      " [44.  ]\n",
      " [58.  ]\n",
      " [28.  ]\n",
      " [42.  ]\n",
      " [28.  ]\n",
      " [24.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [34.  ]\n",
      " [45.5 ]\n",
      " [18.  ]\n",
      " [ 2.  ]\n",
      " [32.  ]\n",
      " [26.  ]\n",
      " [16.  ]\n",
      " [40.  ]\n",
      " [24.  ]\n",
      " [35.  ]\n",
      " [22.  ]\n",
      " [30.  ]\n",
      " [28.  ]\n",
      " [31.  ]\n",
      " [27.  ]\n",
      " [42.  ]\n",
      " [32.  ]\n",
      " [30.  ]\n",
      " [16.  ]\n",
      " [27.  ]\n",
      " [51.  ]\n",
      " [28.  ]\n",
      " [38.  ]\n",
      " [22.  ]\n",
      " [19.  ]\n",
      " [20.5 ]\n",
      " [18.  ]\n",
      " [28.  ]\n",
      " [35.  ]\n",
      " [29.  ]\n",
      " [59.  ]\n",
      " [ 5.  ]\n",
      " [24.  ]\n",
      " [28.  ]\n",
      " [44.  ]\n",
      " [ 8.  ]\n",
      " [19.  ]\n",
      " [33.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [29.  ]\n",
      " [22.  ]\n",
      " [30.  ]\n",
      " [44.  ]\n",
      " [25.  ]\n",
      " [24.  ]\n",
      " [37.  ]\n",
      " [54.  ]\n",
      " [28.  ]\n",
      " [29.  ]\n",
      " [62.  ]\n",
      " [30.  ]\n",
      " [41.  ]\n",
      " [29.  ]\n",
      " [28.  ]\n",
      " [30.  ]\n",
      " [35.  ]\n",
      " [50.  ]\n",
      " [28.  ]\n",
      " [ 3.  ]\n",
      " [52.  ]\n",
      " [40.  ]\n",
      " [28.  ]\n",
      " [36.  ]\n",
      " [16.  ]\n",
      " [25.  ]\n",
      " [58.  ]\n",
      " [35.  ]\n",
      " [28.  ]\n",
      " [25.  ]\n",
      " [41.  ]\n",
      " [37.  ]\n",
      " [28.  ]\n",
      " [63.  ]\n",
      " [45.  ]\n",
      " [28.  ]\n",
      " [ 7.  ]\n",
      " [35.  ]\n",
      " [65.  ]\n",
      " [28.  ]\n",
      " [16.  ]\n",
      " [19.  ]\n",
      " [28.  ]\n",
      " [33.  ]\n",
      " [30.  ]\n",
      " [22.  ]\n",
      " [42.  ]\n",
      " [22.  ]\n",
      " [26.  ]\n",
      " [19.  ]\n",
      " [36.  ]\n",
      " [24.  ]\n",
      " [24.  ]\n",
      " [28.  ]\n",
      " [23.5 ]\n",
      " [ 2.  ]\n",
      " [28.  ]\n",
      " [50.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [19.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [ 0.92]\n",
      " [28.  ]\n",
      " [17.  ]\n",
      " [30.  ]\n",
      " [30.  ]\n",
      " [24.  ]\n",
      " [18.  ]\n",
      " [26.  ]\n",
      " [28.  ]\n",
      " [43.  ]\n",
      " [26.  ]\n",
      " [24.  ]\n",
      " [54.  ]\n",
      " [31.  ]\n",
      " [40.  ]\n",
      " [22.  ]\n",
      " [27.  ]\n",
      " [30.  ]\n",
      " [22.  ]\n",
      " [28.  ]\n",
      " [36.  ]\n",
      " [61.  ]\n",
      " [36.  ]\n",
      " [31.  ]\n",
      " [16.  ]\n",
      " [28.  ]\n",
      " [45.5 ]\n",
      " [38.  ]\n",
      " [16.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [29.  ]\n",
      " [41.  ]\n",
      " [45.  ]\n",
      " [45.  ]\n",
      " [ 2.  ]\n",
      " [24.  ]\n",
      " [28.  ]\n",
      " [25.  ]\n",
      " [36.  ]\n",
      " [24.  ]\n",
      " [40.  ]\n",
      " [28.  ]\n",
      " [ 3.  ]\n",
      " [42.  ]\n",
      " [23.  ]\n",
      " [28.  ]\n",
      " [15.  ]\n",
      " [25.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [22.  ]\n",
      " [38.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [40.  ]\n",
      " [29.  ]\n",
      " [45.  ]\n",
      " [35.  ]\n",
      " [28.  ]\n",
      " [30.  ]\n",
      " [60.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [24.  ]\n",
      " [25.  ]\n",
      " [18.  ]\n",
      " [19.  ]\n",
      " [22.  ]\n",
      " [ 3.  ]\n",
      " [28.  ]\n",
      " [22.  ]\n",
      " [27.  ]\n",
      " [20.  ]\n",
      " [19.  ]\n",
      " [42.  ]\n",
      " [ 1.  ]\n",
      " [32.  ]\n",
      " [35.  ]\n",
      " [28.  ]\n",
      " [18.  ]\n",
      " [ 1.  ]\n",
      " [36.  ]\n",
      " [28.  ]\n",
      " [17.  ]\n",
      " [36.  ]\n",
      " [21.  ]\n",
      " [28.  ]\n",
      " [23.  ]\n",
      " [24.  ]\n",
      " [22.  ]\n",
      " [31.  ]\n",
      " [46.  ]\n",
      " [23.  ]\n",
      " [28.  ]\n",
      " [39.  ]\n",
      " [26.  ]\n",
      " [21.  ]\n",
      " [28.  ]\n",
      " [20.  ]\n",
      " [34.  ]\n",
      " [51.  ]\n",
      " [ 3.  ]\n",
      " [21.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [33.  ]\n",
      " [28.  ]\n",
      " [44.  ]\n",
      " [28.  ]\n",
      " [34.  ]\n",
      " [18.  ]\n",
      " [30.  ]\n",
      " [10.  ]\n",
      " [28.  ]\n",
      " [21.  ]\n",
      " [29.  ]\n",
      " [28.  ]\n",
      " [18.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [19.  ]\n",
      " [28.  ]\n",
      " [32.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [42.  ]\n",
      " [17.  ]\n",
      " [50.  ]\n",
      " [14.  ]\n",
      " [21.  ]\n",
      " [24.  ]\n",
      " [64.  ]\n",
      " [31.  ]\n",
      " [45.  ]\n",
      " [20.  ]\n",
      " [25.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [ 4.  ]\n",
      " [13.  ]\n",
      " [34.  ]\n",
      " [ 5.  ]\n",
      " [52.  ]\n",
      " [36.  ]\n",
      " [28.  ]\n",
      " [30.  ]\n",
      " [49.  ]\n",
      " [28.  ]\n",
      " [29.  ]\n",
      " [65.  ]\n",
      " [28.  ]\n",
      " [50.  ]\n",
      " [28.  ]\n",
      " [48.  ]\n",
      " [34.  ]\n",
      " [47.  ]\n",
      " [48.  ]\n",
      " [28.  ]\n",
      " [38.  ]\n",
      " [28.  ]\n",
      " [56.  ]\n",
      " [28.  ]\n",
      " [ 0.75]\n",
      " [28.  ]\n",
      " [38.  ]\n",
      " [33.  ]\n",
      " [23.  ]\n",
      " [22.  ]\n",
      " [28.  ]\n",
      " [34.  ]\n",
      " [29.  ]\n",
      " [22.  ]\n",
      " [ 2.  ]\n",
      " [ 9.  ]\n",
      " [28.  ]\n",
      " [50.  ]\n",
      " [63.  ]\n",
      " [25.  ]\n",
      " [28.  ]\n",
      " [35.  ]\n",
      " [58.  ]\n",
      " [30.  ]\n",
      " [ 9.  ]\n",
      " [28.  ]\n",
      " [21.  ]\n",
      " [55.  ]\n",
      " [71.  ]\n",
      " [21.  ]\n",
      " [28.  ]\n",
      " [54.  ]\n",
      " [28.  ]\n",
      " [25.  ]\n",
      " [24.  ]\n",
      " [17.  ]\n",
      " [21.  ]\n",
      " [28.  ]\n",
      " [37.  ]\n",
      " [16.  ]\n",
      " [18.  ]\n",
      " [33.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [26.  ]\n",
      " [29.  ]\n",
      " [28.  ]\n",
      " [36.  ]\n",
      " [54.  ]\n",
      " [24.  ]\n",
      " [47.  ]\n",
      " [34.  ]\n",
      " [28.  ]\n",
      " [36.  ]\n",
      " [32.  ]\n",
      " [30.  ]\n",
      " [22.  ]\n",
      " [28.  ]\n",
      " [44.  ]\n",
      " [28.  ]\n",
      " [40.5 ]\n",
      " [50.  ]\n",
      " [28.  ]\n",
      " [39.  ]\n",
      " [23.  ]\n",
      " [ 2.  ]\n",
      " [28.  ]\n",
      " [17.  ]\n",
      " [28.  ]\n",
      " [30.  ]\n",
      " [ 7.  ]\n",
      " [45.  ]\n",
      " [30.  ]\n",
      " [28.  ]\n",
      " [22.  ]\n",
      " [36.  ]\n",
      " [ 9.  ]\n",
      " [11.  ]\n",
      " [32.  ]\n",
      " [50.  ]\n",
      " [64.  ]\n",
      " [19.  ]\n",
      " [28.  ]\n",
      " [33.  ]\n",
      " [ 8.  ]\n",
      " [17.  ]\n",
      " [27.  ]\n",
      " [28.  ]\n",
      " [22.  ]\n",
      " [22.  ]\n",
      " [62.  ]\n",
      " [48.  ]\n",
      " [28.  ]\n",
      " [39.  ]\n",
      " [36.  ]\n",
      " [28.  ]\n",
      " [40.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [24.  ]\n",
      " [19.  ]\n",
      " [29.  ]\n",
      " [28.  ]\n",
      " [32.  ]\n",
      " [62.  ]\n",
      " [53.  ]\n",
      " [36.  ]\n",
      " [28.  ]\n",
      " [16.  ]\n",
      " [19.  ]\n",
      " [34.  ]\n",
      " [39.  ]\n",
      " [28.  ]\n",
      " [32.  ]\n",
      " [25.  ]\n",
      " [39.  ]\n",
      " [54.  ]\n",
      " [36.  ]\n",
      " [28.  ]\n",
      " [18.  ]\n",
      " [47.  ]\n",
      " [60.  ]\n",
      " [22.  ]\n",
      " [28.  ]\n",
      " [35.  ]\n",
      " [52.  ]\n",
      " [47.  ]\n",
      " [28.  ]\n",
      " [37.  ]\n",
      " [36.  ]\n",
      " [28.  ]\n",
      " [49.  ]\n",
      " [28.  ]\n",
      " [49.  ]\n",
      " [24.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [44.  ]\n",
      " [35.  ]\n",
      " [36.  ]\n",
      " [30.  ]\n",
      " [27.  ]\n",
      " [22.  ]\n",
      " [40.  ]\n",
      " [39.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [35.  ]\n",
      " [24.  ]\n",
      " [34.  ]\n",
      " [26.  ]\n",
      " [ 4.  ]\n",
      " [26.  ]\n",
      " [27.  ]\n",
      " [42.  ]\n",
      " [20.  ]\n",
      " [21.  ]\n",
      " [21.  ]\n",
      " [61.  ]\n",
      " [57.  ]\n",
      " [21.  ]\n",
      " [26.  ]\n",
      " [28.  ]\n",
      " [80.  ]\n",
      " [51.  ]\n",
      " [32.  ]\n",
      " [28.  ]\n",
      " [ 9.  ]\n",
      " [28.  ]\n",
      " [32.  ]\n",
      " [31.  ]\n",
      " [41.  ]\n",
      " [28.  ]\n",
      " [20.  ]\n",
      " [24.  ]\n",
      " [ 2.  ]\n",
      " [28.  ]\n",
      " [ 0.75]\n",
      " [48.  ]\n",
      " [19.  ]\n",
      " [56.  ]\n",
      " [28.  ]\n",
      " [23.  ]\n",
      " [28.  ]\n",
      " [18.  ]\n",
      " [21.  ]\n",
      " [28.  ]\n",
      " [18.  ]\n",
      " [24.  ]\n",
      " [28.  ]\n",
      " [32.  ]\n",
      " [23.  ]\n",
      " [58.  ]\n",
      " [50.  ]\n",
      " [40.  ]\n",
      " [47.  ]\n",
      " [36.  ]\n",
      " [20.  ]\n",
      " [32.  ]\n",
      " [25.  ]\n",
      " [28.  ]\n",
      " [43.  ]\n",
      " [28.  ]\n",
      " [40.  ]\n",
      " [31.  ]\n",
      " [70.  ]\n",
      " [31.  ]\n",
      " [28.  ]\n",
      " [18.  ]\n",
      " [24.5 ]\n",
      " [18.  ]\n",
      " [43.  ]\n",
      " [36.  ]\n",
      " [28.  ]\n",
      " [27.  ]\n",
      " [20.  ]\n",
      " [14.  ]\n",
      " [60.  ]\n",
      " [25.  ]\n",
      " [14.  ]\n",
      " [19.  ]\n",
      " [18.  ]\n",
      " [15.  ]\n",
      " [31.  ]\n",
      " [ 4.  ]\n",
      " [28.  ]\n",
      " [25.  ]\n",
      " [60.  ]\n",
      " [52.  ]\n",
      " [44.  ]\n",
      " [28.  ]\n",
      " [49.  ]\n",
      " [42.  ]\n",
      " [18.  ]\n",
      " [35.  ]\n",
      " [18.  ]\n",
      " [25.  ]\n",
      " [26.  ]\n",
      " [39.  ]\n",
      " [45.  ]\n",
      " [42.  ]\n",
      " [22.  ]\n",
      " [28.  ]\n",
      " [24.  ]\n",
      " [28.  ]\n",
      " [48.  ]\n",
      " [29.  ]\n",
      " [52.  ]\n",
      " [19.  ]\n",
      " [38.  ]\n",
      " [27.  ]\n",
      " [28.  ]\n",
      " [33.  ]\n",
      " [ 6.  ]\n",
      " [17.  ]\n",
      " [34.  ]\n",
      " [50.  ]\n",
      " [27.  ]\n",
      " [20.  ]\n",
      " [30.  ]\n",
      " [28.  ]\n",
      " [25.  ]\n",
      " [25.  ]\n",
      " [29.  ]\n",
      " [11.  ]\n",
      " [28.  ]\n",
      " [23.  ]\n",
      " [23.  ]\n",
      " [28.5 ]\n",
      " [48.  ]\n",
      " [35.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [36.  ]\n",
      " [21.  ]\n",
      " [24.  ]\n",
      " [31.  ]\n",
      " [70.  ]\n",
      " [16.  ]\n",
      " [30.  ]\n",
      " [19.  ]\n",
      " [31.  ]\n",
      " [ 4.  ]\n",
      " [ 6.  ]\n",
      " [33.  ]\n",
      " [23.  ]\n",
      " [48.  ]\n",
      " [ 0.67]\n",
      " [28.  ]\n",
      " [18.  ]\n",
      " [34.  ]\n",
      " [33.  ]\n",
      " [28.  ]\n",
      " [41.  ]\n",
      " [20.  ]\n",
      " [36.  ]\n",
      " [16.  ]\n",
      " [51.  ]\n",
      " [28.  ]\n",
      " [30.5 ]\n",
      " [28.  ]\n",
      " [32.  ]\n",
      " [24.  ]\n",
      " [48.  ]\n",
      " [57.  ]\n",
      " [28.  ]\n",
      " [54.  ]\n",
      " [18.  ]\n",
      " [28.  ]\n",
      " [ 5.  ]\n",
      " [28.  ]\n",
      " [43.  ]\n",
      " [13.  ]\n",
      " [17.  ]\n",
      " [29.  ]\n",
      " [28.  ]\n",
      " [25.  ]\n",
      " [25.  ]\n",
      " [18.  ]\n",
      " [ 8.  ]\n",
      " [ 1.  ]\n",
      " [46.  ]\n",
      " [28.  ]\n",
      " [16.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [25.  ]\n",
      " [39.  ]\n",
      " [49.  ]\n",
      " [31.  ]\n",
      " [30.  ]\n",
      " [30.  ]\n",
      " [34.  ]\n",
      " [31.  ]\n",
      " [11.  ]\n",
      " [ 0.42]\n",
      " [27.  ]\n",
      " [31.  ]\n",
      " [39.  ]\n",
      " [18.  ]\n",
      " [39.  ]\n",
      " [33.  ]\n",
      " [26.  ]\n",
      " [39.  ]\n",
      " [35.  ]\n",
      " [ 6.  ]\n",
      " [30.5 ]\n",
      " [28.  ]\n",
      " [23.  ]\n",
      " [31.  ]\n",
      " [43.  ]\n",
      " [10.  ]\n",
      " [52.  ]\n",
      " [27.  ]\n",
      " [38.  ]\n",
      " [27.  ]\n",
      " [ 2.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [ 1.  ]\n",
      " [28.  ]\n",
      " [15.  ]\n",
      " [ 0.83]\n",
      " [28.  ]\n",
      " [23.  ]\n",
      " [18.  ]\n",
      " [39.  ]\n",
      " [21.  ]\n",
      " [28.  ]\n",
      " [32.  ]\n",
      " [28.  ]\n",
      " [20.  ]\n",
      " [16.  ]\n",
      " [30.  ]\n",
      " [34.5 ]\n",
      " [17.  ]\n",
      " [42.  ]\n",
      " [28.  ]\n",
      " [35.  ]\n",
      " [28.  ]\n",
      " [28.  ]\n",
      " [ 4.  ]\n",
      " [74.  ]\n",
      " [ 9.  ]\n",
      " [16.  ]\n",
      " [44.  ]\n",
      " [18.  ]\n",
      " [45.  ]\n",
      " [51.  ]\n",
      " [24.  ]\n",
      " [28.  ]\n",
      " [41.  ]\n",
      " [21.  ]\n",
      " [48.  ]\n",
      " [28.  ]\n",
      " [24.  ]\n",
      " [42.  ]\n",
      " [27.  ]\n",
      " [31.  ]\n",
      " [28.  ]\n",
      " [ 4.  ]\n",
      " [26.  ]\n",
      " [47.  ]\n",
      " [33.  ]\n",
      " [47.  ]\n",
      " [28.  ]\n",
      " [15.  ]\n",
      " [20.  ]\n",
      " [19.  ]\n",
      " [28.  ]\n",
      " [56.  ]\n",
      " [25.  ]\n",
      " [33.  ]\n",
      " [22.  ]\n",
      " [28.  ]\n",
      " [25.  ]\n",
      " [39.  ]\n",
      " [27.  ]\n",
      " [19.  ]\n",
      " [28.  ]\n",
      " [26.  ]\n",
      " [32.  ]]\n"
     ]
    }
   ],
   "source": [
    "impute = SimpleImputer(strategy=\"median\")\n",
    "ages = np.array(nec_data[\"Age\"]).reshape(-1, 1)\n",
    "impute.fit(ages)\n",
    "no_nan_age = impute.transform(ages)\n",
    "print(no_nan_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.0\n",
      "1      0.0\n",
      "2      0.0\n",
      "3      0.0\n",
      "4      0.0\n",
      "      ... \n",
      "886    0.0\n",
      "887    0.0\n",
      "888    0.0\n",
      "889    0.0\n",
      "890    0.0\n",
      "Name: is_child, Length: 889, dtype: float64\n",
      "0      1.0\n",
      "1      1.0\n",
      "2      1.0\n",
      "3      1.0\n",
      "4      1.0\n",
      "      ... \n",
      "886    1.0\n",
      "887    1.0\n",
      "888    1.0\n",
      "889    1.0\n",
      "890    1.0\n",
      "Name: is_adult, Length: 889, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "nec_data[\"is_child\"] = (no_nan_age < 18).astype(float)\n",
    "nec_data[\"is_adult\"] = (no_nan_age >= 18).astype(float)\n",
    "print(nec_data[\"is_child\"])\n",
    "print(nec_data[\"is_adult\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>is_child</th>\n",
       "      <th>is_adult</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Q</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>S</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>S</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>S</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>C</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age  SibSp  Parch     Fare Embarked  is_child  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S       0.0   \n",
       "1         1       1  female  38.0      1      0  71.2833        C       0.0   \n",
       "2         1       3  female  26.0      0      0   7.9250        S       0.0   \n",
       "3         1       1  female  35.0      1      0  53.1000        S       0.0   \n",
       "4         0       3    male  35.0      0      0   8.0500        S       0.0   \n",
       "5         0       3    male   NaN      0      0   8.4583        Q       0.0   \n",
       "6         0       1    male  54.0      0      0  51.8625        S       0.0   \n",
       "7         0       3    male   2.0      3      1  21.0750        S       1.0   \n",
       "8         1       3  female  27.0      0      2  11.1333        S       0.0   \n",
       "9         1       2  female  14.0      1      0  30.0708        C       1.0   \n",
       "\n",
       "   is_adult  \n",
       "0       1.0  \n",
       "1       1.0  \n",
       "2       1.0  \n",
       "3       1.0  \n",
       "4       1.0  \n",
       "5       1.0  \n",
       "6       1.0  \n",
       "7       0.0  \n",
       "8       1.0  \n",
       "9       0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nec_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.825209</td>\n",
       "      <td>-0.563674</td>\n",
       "      <td>0.431350</td>\n",
       "      <td>-0.474326</td>\n",
       "      <td>-0.500240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.572211</td>\n",
       "      <td>0.669217</td>\n",
       "      <td>0.431350</td>\n",
       "      <td>-0.474326</td>\n",
       "      <td>0.788947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.825209</td>\n",
       "      <td>-0.255451</td>\n",
       "      <td>-0.475199</td>\n",
       "      <td>-0.474326</td>\n",
       "      <td>-0.486650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.572211</td>\n",
       "      <td>0.438050</td>\n",
       "      <td>0.431350</td>\n",
       "      <td>-0.474326</td>\n",
       "      <td>0.422861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.825209</td>\n",
       "      <td>0.438050</td>\n",
       "      <td>-0.475199</td>\n",
       "      <td>-0.474326</td>\n",
       "      <td>-0.484133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.825209</td>\n",
       "      <td>-0.101340</td>\n",
       "      <td>-0.475199</td>\n",
       "      <td>-0.474326</td>\n",
       "      <td>-0.475913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.572211</td>\n",
       "      <td>1.902108</td>\n",
       "      <td>-0.475199</td>\n",
       "      <td>-0.474326</td>\n",
       "      <td>0.397946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.825209</td>\n",
       "      <td>-2.104788</td>\n",
       "      <td>2.244449</td>\n",
       "      <td>0.765897</td>\n",
       "      <td>-0.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.825209</td>\n",
       "      <td>-0.178396</td>\n",
       "      <td>-0.475199</td>\n",
       "      <td>2.006119</td>\n",
       "      <td>-0.422057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.373501</td>\n",
       "      <td>-1.180120</td>\n",
       "      <td>0.431350</td>\n",
       "      <td>-0.474326</td>\n",
       "      <td>-0.040787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7         8         9         10  \\\n",
       "0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  0.825209 -0.563674  0.431350   \n",
       "1  1.0  0.0  1.0  0.0  0.0  1.0  0.0  1.0 -1.572211  0.669217  0.431350   \n",
       "2  1.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0  0.825209 -0.255451 -0.475199   \n",
       "3  1.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0 -1.572211  0.438050  0.431350   \n",
       "4  0.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  0.825209  0.438050 -0.475199   \n",
       "5  0.0  1.0  0.0  1.0  0.0  0.0  0.0  1.0  0.825209 -0.101340 -0.475199   \n",
       "6  0.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0 -1.572211  1.902108 -0.475199   \n",
       "7  0.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0  0.825209 -2.104788  2.244449   \n",
       "8  1.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0  0.825209 -0.178396 -0.475199   \n",
       "9  1.0  0.0  1.0  0.0  0.0  1.0  1.0  0.0 -0.373501 -1.180120  0.431350   \n",
       "\n",
       "         11        12  \n",
       "0 -0.474326 -0.500240  \n",
       "1 -0.474326  0.788947  \n",
       "2 -0.474326 -0.486650  \n",
       "3 -0.474326  0.422861  \n",
       "4 -0.474326 -0.484133  \n",
       "5 -0.474326 -0.475913  \n",
       "6 -0.474326  0.397946  \n",
       "7  0.765897 -0.221900  \n",
       "8  2.006119 -0.422057  \n",
       "9 -0.474326 -0.040787  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline = make_pipeline(SimpleImputer(strategy=\"most_frequent\"), OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "num_norm_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "nothing = make_pipeline(SimpleImputer(strategy=\"median\"))\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"text\", text_pipeline, make_column_selector(dtype_include=object)),\n",
    "    (\"nothing\", nothing, [\"Survived\", \"is_child\", \"is_adult\"])],\n",
    "    remainder=num_norm_pipeline\n",
    ")\n",
    "pre_nec_data = preprocess.fit_transform(nec_data)\n",
    "pre_nec_data = pd.DataFrame(pre_nec_data)\n",
    "pre_nec_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above order is is_female, is_male, from_Cherbourg(C), from_Queenstown(Q), from_Southampton(S), Survived, is_child, is_adult, Pclass, Age, SibSp, Parch, Fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_female, 0.5415849155511678\n",
      "is_male, -0.5415849155511681\n",
      "from_Cherbourg(C), 0.16996596681270024\n",
      "from_Queenstown(Q), 0.00453572872398569\n",
      "from_Southampton(S), -0.1517770485943329\n",
      "Survived, 1.0\n",
      "is_child, 0.12356875935781529\n",
      "is_adult, -0.1235687593578151\n",
      "Pclass, -0.33554885935682505\n",
      "Age, -0.06982170767891915\n",
      "SibSp, -0.03403999879674894\n",
      "Parch, 0.0831507836662021\n",
      "Fare, 0.2552904613046991\n"
     ]
    }
   ],
   "source": [
    "nec_matrix = pre_nec_data.corr()\n",
    "nec_titles = [\"is_female\", \"is_male\", \"from_Cherbourg(C)\", \"from_Queenstown(Q)\", \"from_Southampton(S)\", \"Survived\", \"is_child\", \"is_adult\", \"Pclass\", \n",
    "                               \"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "for k in range(len(nec_titles)):\n",
    "    print(f\"{nec_titles[k]}, {nec_matrix[5][k]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is_child and is_adult are seemingly not highly correlated. There's one more way I can test this, along with feature importances in general: a random tree classifier and its feature_importances variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_log(x):\n",
    "    return np.log(x + 1e-10)\n",
    "#thanks to ChatGPT 3.5 for this function\n",
    "\n",
    "log_transformer = FunctionTransformer(func=safe_log, inverse_func=np.exp)\n",
    "num_tail_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), log_transformer, StandardScaler())\n",
    "preprocess2 = ColumnTransformer([\n",
    "    (\"tail\", num_tail_pipeline, [\"SibSp\", \"Parch\", \"Fare\"]), # not for Pclass, since it's basically ordinally encoded\n",
    "    (\"nothing\", nothing, [\"is_child\", \"is_adult\"]),\n",
    "    (\"text\", text_pipeline, make_column_selector(dtype_include=object))],\n",
    "    remainder=num_norm_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42826843445747337\n"
     ]
    }
   ],
   "source": [
    "nec_data = nec_data.drop(\"Survived\", axis=1)\n",
    "\n",
    "rdf_clf = Pipeline([(\"pre\", preprocess2), (\"clf\", RandomForestClassifier(random_state=446))])\n",
    "rdf_clf.fit(nec_data, survived)\n",
    "rmse = -cross_val_score(rdf_clf, nec_data, survived, scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "print(np.average(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SibSp, 0.05135387326148228\n",
      "Parch, 0.036776002089142494\n",
      "Fare, 0.2515163785245415\n",
      "is_child, 0.010805475678611573\n",
      "is_adult, 0.010062469890951494\n",
      "is_female, 0.1318763679221925\n",
      "is_male, 0.1562505432179034\n",
      "from_Cherbourg(C), 0.013197921840731716\n",
      "from_Queenstown(Q), 0.007129719576922866\n",
      "from_Southampton(S), 0.012221833164927231\n",
      "Pclass, 0.08755093670517926\n",
      "Age, 0.23125847812741376\n"
     ]
    }
   ],
   "source": [
    "nec_features = rdf_clf[\"clf\"].feature_importances_\n",
    "nec_titles = [\"SibSp\", \"Parch\", \"Fare\", \"is_child\", \"is_adult\", \"is_female\", \"is_male\", \"from_Cherbourg(C)\", \"from_Queenstown(Q)\", \"from_Southampton(S)\", \n",
    "              \"Pclass\", \"Age\"]\n",
    "for k in range(len(nec_titles)):\n",
    "    print(f\"{nec_titles[k]}, {nec_features[k]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, is_child and is_adult were pretty insignificant. Gender and Fare remained important, and even Age is important here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_use = test_set.copy()\n",
    "# impute = SimpleImputer(strategy=\"median\")\n",
    "# ages = np.array(test_use[\"Age\"]).reshape(-1, 1)\n",
    "# impute.fit(ages)\n",
    "# no_nan_age = impute.transform(ages)\n",
    "# test_use[\"is_child\"] = (no_nan_age < 18).astype(float)\n",
    "# test_use[\"is_adult\"] = (no_nan_age >= 18).astype(float)\n",
    "\n",
    "\n",
    "# test_pred = rdf_clf.predict(test_use)\n",
    "# with open('sacreddeer_titanic_new_submission_5.csv', 'w', newline='') as f:\n",
    "#     # create the csv writer\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow([\"PassengerId\", \"Survived\"])\n",
    "#     for k in range(len(ids2)):\n",
    "#         # write a row to the csv file\n",
    "#         writer.writerow([ids2[k], test_pred[k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4655153270478422\n"
     ]
    }
   ],
   "source": [
    "imp_data = nec_data[[\"Fare\", \"Sex\", \"Age\"]]\n",
    "\n",
    "preprocess3 = ColumnTransformer([\n",
    "    (\"tail\", num_tail_pipeline, [\"Fare\"]), # not for Pclass, since it's basically ordinally encode\n",
    "    (\"text\", text_pipeline, make_column_selector(dtype_include=object))],\n",
    "    remainder=num_norm_pipeline\n",
    ")\n",
    "\n",
    "rdf_clf2 = Pipeline([(\"pre\", preprocess3), (\"clf\", RandomForestClassifier(random_state=446))])\n",
    "rdf_clf2.fit(imp_data, survived)\n",
    "rmse = -cross_val_score(rdf_clf2, imp_data, survived, scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "print(np.average(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4612560463684951\n"
     ]
    }
   ],
   "source": [
    "imp_data2 = nec_data[[\"Sex\"]]\n",
    "\n",
    "preprocess4 = ColumnTransformer([\n",
    "    (\"text\", text_pipeline, make_column_selector(dtype_include=object))],\n",
    "    remainder=num_norm_pipeline\n",
    ")\n",
    "\n",
    "rdf_clf3 = Pipeline([(\"pre\", preprocess4), (\"clf\", RandomForestClassifier(random_state=446))])\n",
    "rdf_clf3.fit(imp_data2, survived)\n",
    "rmse = -cross_val_score(rdf_clf3, imp_data2, survived, scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "print(np.average(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both attempts at removing features for generalization seem to fail, as their RMSE is even worse than the original random forest. My last option is hyperparameter tuning via random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Pipeline.get_params of Pipeline(steps=[('pre',\n",
      "                 ColumnTransformer(remainder=Pipeline(steps=[('simpleimputer',\n",
      "                                                              SimpleImputer(strategy='median')),\n",
      "                                                             ('standardscaler',\n",
      "                                                              StandardScaler())]),\n",
      "                                   transformers=[('tail',\n",
      "                                                  Pipeline(steps=[('simpleimputer',\n",
      "                                                                   SimpleImputer(strategy='median')),\n",
      "                                                                  ('functiontransformer',\n",
      "                                                                   FunctionTransformer(func=<function safe_log at 0x0000015ABB894B80>,\n",
      "                                                                                       inverse_func=<ufun...\n",
      "                                                                   SimpleImputer(strategy='median'))]),\n",
      "                                                  ['is_child', 'is_adult']),\n",
      "                                                 ('text',\n",
      "                                                  Pipeline(steps=[('simpleimputer',\n",
      "                                                                   SimpleImputer(strategy='most_frequent')),\n",
      "                                                                  ('onehotencoder',\n",
      "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                                  <sklearn.compose._column_transformer.make_column_selector object at 0x0000015ABB919BA0>)])),\n",
      "                ('clf',\n",
      "                 RandomForestClassifier(max_depth=8, n_estimators=227,\n",
      "                                        random_state=446))])>\n",
      "0.39628591670470675\n"
     ]
    }
   ],
   "source": [
    "randdepth = np.array([3, 4, 5, 6, 7, 8, 9])\n",
    "param_distribs = {\"clf__max_depth\": randdepth,\n",
    "                  \"clf__min_samples_leaf\": randint(low=1, high=10),\n",
    "                  \"clf__n_estimators\": randint(low=200, high=230)}\n",
    "rmd_search = RandomizedSearchCV(\n",
    "    rdf_clf, param_distributions=param_distribs, \n",
    "    n_iter=20, cv=3, scoring=\"neg_root_mean_squared_error\", random_state=446\n",
    ")\n",
    "rmd_search.fit(nec_data, survived)\n",
    "final_rnd_model = rmd_search.best_estimator_\n",
    "print(final_rnd_model.get_params)\n",
    "\n",
    "final_rnd_model.fit(nec_data, survived)\n",
    "rmse2 = -cross_val_score(final_rnd_model, nec_data, survived, scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "print(np.average(rmse2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_use = test_set.copy()\n",
    "# impute = SimpleImputer(strategy=\"median\")\n",
    "# ages = np.array(test_use[\"Age\"]).reshape(-1, 1)\n",
    "# impute.fit(ages)\n",
    "# no_nan_age = impute.transform(ages)\n",
    "# test_use[\"is_child\"] = (no_nan_age < 18).astype(float)\n",
    "# test_use[\"is_adult\"] = (no_nan_age >= 18).astype(float)\n",
    "\n",
    "\n",
    "# test_pred = final_rnd_model.predict(test_use)\n",
    "# with open('sacreddeer_titanic_new_submission_6.csv', 'w', newline='') as f:\n",
    "#     # create the csv writer\n",
    "#     writer = csv.writer(f)\n",
    "#     writer.writerow([\"PassengerId\", \"Survived\"])\n",
    "#     for k in range(len(ids2)):\n",
    "#         # write a row to the csv file\n",
    "#         writer.writerow([ids2[k], test_pred[k]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This got me the highest score I've ever had, 0.7790. This boosted me up to 2835th place (as of 9/19/23) out of 14653 teams (although the person in 2305th place also has my score). I'm going to see how AdaBoost does on the data - is_child and is_adult removed - and then move onto a Kaggle housing prices regression competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 889 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Pclass    889 non-null    int64  \n",
      " 1   Sex       889 non-null    object \n",
      " 2   Age       712 non-null    float64\n",
      " 3   SibSp     889 non-null    int64  \n",
      " 4   Parch     889 non-null    int64  \n",
      " 5   Fare      889 non-null    float64\n",
      " 6   Embarked  889 non-null    object \n",
      " 7   is_child  889 non-null    float64\n",
      " 8   is_adult  889 non-null    float64\n",
      "dtypes: float64(4), int64(3), object(2)\n",
      "memory usage: 69.5+ KB\n"
     ]
    }
   ],
   "source": [
    "nec_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nec_data = nec_data.drop(\"is_child\", axis=1)\n",
    "nec_data = nec_data.drop(\"is_adult\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45433170958174757\n"
     ]
    }
   ],
   "source": [
    "preprocess5 = ColumnTransformer([\n",
    "    (\"tail\", num_tail_pipeline, [\"SibSp\", \"Parch\", \"Fare\"]), # not for Pclass, since it's basically ordinally encoded\n",
    "    (\"text\", text_pipeline, make_column_selector(dtype_include=object))],\n",
    "    remainder=num_norm_pipeline\n",
    ")\n",
    "\n",
    "ada_clf = Pipeline([(\"pre\", preprocess5), (\"clf\", AdaBoostClassifier(DecisionTreeClassifier(max_depth=8), random_state=446))])\n",
    "ada_clf.fit(nec_data, survived)\n",
    "rmse = -cross_val_score(ada_clf, nec_data, survived, scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "print(np.average(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=8),\n",
      "                   learning_rate=0.0838948441812057, n_estimators=239,\n",
      "                   random_state=446)\n",
      "0.4336358023356578\n"
     ]
    }
   ],
   "source": [
    "randlr = np.zeros(10)\n",
    "for k in range(len(randlr)):\n",
    "    randlr[k] = random.uniform(0.01, 0.99)\n",
    "param_distribs = {\"clf__n_estimators\": randint(low=100, high=500),\n",
    "                  \"clf__learning_rate\": randlr,\n",
    "                  }\n",
    "rdm_search3 = RandomizedSearchCV(\n",
    "    ada_clf, param_distributions=param_distribs, n_iter=10, cv=5, scoring=\"neg_root_mean_squared_error\"\n",
    ")\n",
    "rdm_search3.fit(nec_data, survived)\n",
    "best_ada_clf = rdm_search3.best_estimator_\n",
    "print(best_ada_clf[\"clf\"])\n",
    "\n",
    "rmse = -cross_val_score(best_ada_clf, nec_data, survived, scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "print(np.average(rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
